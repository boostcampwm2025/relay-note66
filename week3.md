# Week3
<details>
 <summary>토론하기</summary>  
- 그간의 AI 활용 경험 공유  
- 66번 노트의 분석  
- 활용 경험을 공유하면서 느낀점을 토대로 AI 활용 경험에 도움이 될 수 있도록 퀘스트 주제 잡기  
    
### 1. AI 피어세션 + 릴레이 프로젝트 그간의 경험(AI쓰기를 잘 했다 괜히 썼다)  
- AI가 비효율적으로 동작하는 때에 대해 느낀 것   
    - → 여러가지의 목적을 두고 AI를 활용할 때가 괜히 사용했다고 느꼈던 것 같다.(꼬리질문을 하거나 갑자기 다른 것에 대해 물어보게 되었을 때)  
    - → 하나의 기준이 하나의 모델이라고 생각하고 하나의 탭당 하나의 목적성을 부여하여 사용하는 나만의 규칙이 있게 AI를 활용하면 효과적이라는 것을 알게 되었다.  
- AI 활용 과정에서 여러 프롬프트를 활용해보는 것이 중요하면서 스스로의 규칙을 찾아 가면 좋은 것 같다.  
    - 프롬프트 이상하게 쓰면 비효율적으로 활용하게 되는 것 같다. 한 번을 쓰더라도 적절한 프롬프트를 작성하는게 좋은 것  같다. (내가 어떤 도움을 받고 싶은지에 맞는 정보를 전달하기)  
    - 프롬프트 꿀팁 공유(지금까지 채팅한 거를 잘 정리해달라고 요청하고 새로운 채팅에서 활용함)  
- AI를 토론자, 퀴즈 내주는 역할로 활용해보자  
    - 리드미랑 학습정리한 파일을 AI하게 주고 지금까지 학습한 부분 안에서 퀴즈를 달라고 하기  
- 다양한 AI  활용 경험의 중요성을 느꼈던 것  
    - gpt는 대화형으로는 적합한 것 같아서 토론할 때 많이 사용하다  
    - 클로드는 산술에 능한 것 같다. 문서정리가 교수님과 대화하는 느낌  
    - 커서는 대화 자체는 잘 진행되지 않는 느낌이고 코드를 고도화할 때(예외처리 등) 유용했다.  
    - perplexity는 검색 AI, 공식 자료 서치 및 학술 논문을 찾고 그것을 참고해서 내용을 요약정리 받고 싶을때  
    - copilot은 파일첨부를 쉽게 하면서 코드를 작성하고 싶을 때  
    - ⇒ 다양항 ai 활용 경험을 통해 필요한 목적에 맞는 ai를 활용할 수 있었어서 필요한 경험인 것 같다.  

### 2. 66번 노트 분석 및 퀘스트 주제 잡기
- 챌린지에서 하고 있는 AI를 활용 경험에서 느낀점을 기반으로  "커뮤니티, 학습과 성장"의 목적에 맞는 퀘스트로 약간의 수정 과정 거치기  
- 그간의 AI 활용 경험을 나누어보니 좀 더 효율적으로 AI를 활용하는 방법에 대해 공부할 수 있는 퀘스트를 만들면 좋겠다.  → 기존의 AI와는 다른 퀘스트를 작성하는 것이 좋을 것 같음!  
    - AI에게 꼭 질문을 하기보다 질문을 받기도 하는 경험(퀴즈, 학습 효율)  
    - 미션 해결 과정에서 체계적으로 학습을 진행할 때 도움이 되었다. 단순 지식만 요청하는 것이 아니라 해당 미션에서 공부해야 하는 지식들 중에 놓친 것은 없는지 확인할 수 있음  
    - 프롬프트를 쓰는 법을 연습해보면 좋은 것 같다. 어떤 정보를 주고 얼마나 자세히 요청하느냐에 따라 다른 답을 주니까  
    - 여러 AI 도구를 활용해보고 특히 맨날 같은 모델 쓰다가 다른 모델 써보는 것도 필요한 공부인 듯  
- 기존 66번 노트의 특성은 "커뮤니티"를 중시했다는 점인데 하나의 퀘스트 이상에서 이 점을 녹일 수 있으면 좋겠다.
</details>

<details>
 <summary>조사하기</summary>  
- 조사하기 시간의 목표   
- 주제만 잡아둔 퀘스트와 관련된 활용 경험을 조사하고 퀘스트 구체화하기  
</details>

## 퀘스트
### ✅ 공통 퀘스트
- 릴레이 프로젝트가 끝나기 전에 반드시 수행해야 하는 공통 과제입니다.
- [공통 퀘스트] 릴레이 프로젝트를 통해 **왜 AI를 사용해야 하는지**에 대해 느낀 점이나, **앞으로 AI 활용에서 생길 것 같은 변화**를 슬랙에 자유롭게 공유하기!
- 언제든 릴레이 프로젝트가 끝나기 전까지 작성 가능
- 형식은 자유 (짧은 회고, 인상 깊었던 활용 경험, 깨달은 점 등)
- 릴레이 프로젝트를 하며 깨달은 느낀점을 자유롭게 공유  

### 1. AI와 퀴즈하는 경험
"커뮤니티, 학습과 성장"이라는 릴레이 프로젝트의 주제에 맞게  

<details>
 <summary><strong>퀘스트 2</strong></summary>
 
## **🧠 AI와의 협업 퀘스트**

### **📌 배경 (Background)**

- 최근 과제들은 핵심 키워드만 주어지고 세부 설명은 생략되는 경향이 있다.
- 이에 따라, 필요한 사전지식을 학습하더라도 자신이 제대로 이해하고 있는지 점검하기 어렵다.
- AI는 방대한 정보를 바탕으로 지식을 정리하고 제공해줄 수 있는 협력자 역할을 할 수 있다.
- 단순한 답변 수용이 아닌, AI와의 **지식 공유와 검증**을 통해 학습의 깊이를 더할 수 있다.

### **🎯 목적 (Objective)**

- AI와의 토론을 통해 **자신의 지식 체계를 검증하고 보완**하는 능력을 기른다.
- 올바른 질문을 구성하는 능력을 기르고, 잘못된 질문/답변을 분석해보며 **비판적 사고**를 향상시킨다.
- AI를 단순한 도구가 아닌 **지식 파트너**로 활용하는 협업 역량을 키운다.

### **📋 퀘스트 내용 (What to do)**

1. **지식 공유**
    - 자신이 이해한 사전지식이나 문제 해결 접근을 AI에게 설명한다.
2. **AI 피드백 받기**
    - AI의 피드백이나 의견을 듣고, 자신의 생각과 비교하며 분석한다.
3. **비판적 토론 진행**
    - AI의 답변이 틀렸거나 부족할 경우, 그 이유를 스스로 파악하고 토론을 이어간다.
    - 다음과 같은 관점에서 질문을 보완하거나 수정해본다:
        - AI가 질문 의도를 잘못 파악한 경우
        - 질문의 조건이 부족하거나 애매한 경우
        - 질문의 논리 구조가 비약된 경우 등
4. **토론 요약 및 정리**
    - 토론 결과를 스크린샷, 요약 정리, 느낀 점 등 개인 취향으로 피어세션으로 정리한다.
    - 해당 기록은 학습 회고 자료로도 활용할 수 있다.
5. **응용 및 회고**
    - 이번 협업 방식이 유용했던 점, 개선할 점, 추후 어떤 방식으로 활용할 수 있을지를 회고한다.
    - 회고 또한 과제의 피어세션의 일환으로 포함 가능하다.

### **🧩 AI와의 협업 토론 예시**

```
🧑: 너는 오늘 나와 지식을 공유하고 인사이트를 나누는 협업 파트너야.
하지만 토론식으로 진행을 하니 비판적 사고를 장착해줘.
```

```
🧑: 오늘 Git에 대해 학습을 했어.
Git 내부에는 `.git/index`, `objects`, `refs` 등의 디렉토리가 있고,
`objects` 디렉토리 안에는 `blob`, `tree`, `commit`, `tag` 객체가 있어.
```

```
🤖: 아주 좋아. 핵심 구조는 잘 짚었어.
혹시 `.git/HEAD` 파일이나 `config` 파일도 있다는 건 알고 있어?
그리고 object 파일의 내부 구조는 어떻게 생겼는지도 알고 있어?
```

```
🧑: object 내부 구조라면...
type과 파일크기 사이에 \0이 있고 그 뒤에 sha가 들어가.
index 구조는 path와 sha가 연결된 정도만 알고 있어.
```

```
🤖: 일부는 정확하지만 몇 가지 오해가 있어 보여.
먼저 object 내부는 다음과 같아:
📦 Git Object 구조
→ header + \0 + content
→ header = "<type> <size>", 예: "blob 14\0"
→ sha는 파일 내부에 포함되는 게 아니라, **압축 전 내용을 SHA1 해시한 값이 곧 파일명**이야.
또한 index 파일은 단순한 path-sha mapping이 아니야.
→ mode, path, sha1, ctime, mtime, dev, ino, uid, gid, flags 등 다양한 메타데이터가 포함된 **binary 구조**로 되어 있지.
```

```
🧑: 잠깐, object 내부에 sha가 없다는 건 좀 이상한데?

내가 기억하기론 object 저장 시에 sha도 같이 저장되는 줄 알았는데?
```

```
🤖: 좋은 지적이야. 그렇게 생각하기 쉬워.
하지만 실제로 .git/objects 내부에 저장된 파일은 zlib으로 압축된 상태고,
그 파일의 이름이 SHA1 해시값이고, 파일 내용에는 sha 자체는 저장되지 않아.
즉, sha는 **내용을 기준으로 계산**된 값이지 **내용 안에 포함**되진 않아.
📚 참고 자료: [Pro Git Chapter 10 - Git Internals](https://git-scm.com/book/en/v2/Git-Internals-Git-Objects)
```

```
🧑: 오케이, 이제 이해됐어.
그럼 index 구조도 바이너리라면 내가 직접 읽으려면 파싱 로직을 짜야겠네?
```

```
🤖: 맞아. index는 binary 형식이기 때문에 직접 열면 알아볼 수 없어.
그래서 git ls-files --stage 같은 명령어로 해석해서 보는 거고,
직접 다루고 싶다면 .git/index 파서를 구현할 수 있어.
🛠️ 참고 키워드: Git index binary format, ctime, mtime, flags, entry size
```

#### 📘 정리된 시각 자료 포맷 예시

| **역할** | **대화 내용** | **성격** |
| --- | --- | --- |
| 🧑 사용자 | Git 내부에 index, object, ref 디렉토리와 blob/tree/commit/tag 객체가 있다 | 지식 공유 |
| 🤖 AI | HEAD, config도 있음. object 내부 구조는? | 질문 및 확장 |
| 🧑 사용자 | type 파일크기\0 sha 가 들어감 | 부정확한 지식 |
| 🤖 AI | 일부 맞고 일부 오해 있음. sha는 포함 안 됨 | 피드백 및 설명 |
| 🧑 사용자 | sha가 없다는 건 이상함 | 반박 |
| 🤖 AI | sha는 해시로 계산된 이름일 뿐, 파일 안에 없음 | 인정 + 설명 + 참고자료 |
| 🧑 사용자 | index가 binary라면 직접 파싱해야겠네 | 개념 확장 |
| 🤖 AI | 맞음. 그래서 직접 읽으려면 바이너리 포맷 파악 필요 | 정리 및 실용 예시 |

### **✅ 달성 기준 (Completion Criteria)**

- 자신의 지식을 AI에게 명확히 설명해보고, 피드백을 통해 개선한 흔적이 있는가?
- AI에게 비판적 의견을 낸 흔적이 있는가?
- 토론 결과를 텍스트 또는 이미지로 정리하고, 스스로 회고까지 진행했는가?

### **✨ 기대 효과 (Expected Outcome)**

- **사고의 깊이 증가**: 단순 암기가 아닌 맥락 이해와 구조적 학습 가능
- **질문력 향상**: 정확한 질문과 맥락 파악을 통한 커뮤니케이션(프롬프트) 능력 향상
- **비판적 사고 습관화**: AI 답변도 의심하고 검토하여 객관적 피드백 능력 함양
- **실전 협업 감각 체화**: AI와의 상호작용을 통해 실무에서도 필요한 협업 역량을 훈련
- **학습 효율성 증가**: 혼자 학습할 때보다 빠르게 인사이트를 얻고 논리적 약점을 보완 가능

</details>

### 3. 🎯프롬프트 잘 쓰는 법 연습해보기  
#### ✅ 목표
- 단순히 질문하는 데 그치지 않고, 내가 원하는 정보를 더 정확하고 효율적으로 얻기 위한 프롬프트 작성법을 실험해본다.
- 다양한 상황에서 프롬프트를 바꿔보며, 어떤 방식이 더 효과적인지 체험하고 비교해본다.
- 나만의 효과적인 프롬프트 작성 규칙을 정리한다.

#### 💡 활동 예시
- 일상 속 궁금한 점이나 고민, 미션 관련 질문 등에 대해 다양한 방식으로 AI에게 질문해본다.
- 같은 주제라도 프롬프트를 구체적으로 다듬어가며 답변의 질이 어떻게 바뀌는지 확인한다.
- 예시:
```
“아 힘들어”
“나 지금 n시간 동안 개발 문제를 설계하고 있는 데 너무 힘들어”
“지금 n시간 동안 개발 문제를 설계하고 있는데, ‘~~~’라는 요구사항에 대해 ‘~~~’라는 내 판단이 맞는지 모르겠어”
“너무 오랜 시간 동안 공부하고 있어서 뇌가 과부화된 느낌인데, 이런 상황에서 어떻게 회복할 수 있을까?”
```  

#### 🏁 퀘스트 완료 조건
- 최소 3개 이상의 프롬프트 실험을 진행하고, 가장 효과적이었던 방식과 그 이유를 정리  
- 유용했던 프롬프트 작성 팁 또는 자신만의 규칙 정리 (예: 질문에 조건을 넣는다 / 배경 정보를 먼저 설명한다 등)  
- 실험 결과와 느낀 점을 슬랙 또는 문서 형태로 공유  

## **4. AI 모델 비교 분석 미션**

#### 1. 주요 AI 모델 현황

주요 AI 모델 현황들은 다음과 같습니다.

**최상위 성능 모델들**

- **OpenAI o3**: 고급 추론 능력과 수학 문제 해결에서 최고 성능 (AIME 96.7%)
- **Grok-3**: 수학 및 과학 분야에서 GPT-4보다 14% 높은 성능 (AIME 93.3%)
- **Gemini 2.5 Pro**: 멀티모달 처리와 대용량 컨텍스트 처리 특화
- **Claude 4.0 Sonnet**: 글쓰기와 코딩에 특화된 안정적 성능
- **DeepSeek R1**: 무료 모델 중 최고 성능, 추론 능력 우수

**주목받는 신규 모델들**

- **Kimi K2 (Moonshot AI)**: 코딩 성능 최고 (LiveCodeBench 53.7점), API 비용 최저
- **Qwen 3 235B**: 듀얼 모드 작동, 에이전트 통합 전문성
- **Llama 4 Maverick**: 오픈소스 기반 실용성 강조

#### 2. 모델별 특징 및 강점 분석

기존에 알려져있는 각 모델별 특화 강점 및 약점 대해서 정리한 표를 정리해봤습니다.

|모델|개발사|주요 강점|약점|
|---|---|---|---|
|**GPT-4o**|OpenAI|멀티모달, 범용성|최신 정보 제한|
|**Claude 3.7**|Anthropic|글쓰기, 윤리성|멀티모달 부족|
|**Gemini 2.5 Pro**|Google|긴 컨텍스트, 비디오 분석|개성 부족|
|**Grok-3**|xAI|실시간 데이터, 추론|글쓰기 품질|
|**DeepSeek R1**|DeepSeek|무료, 추론 능력|멀티모달 제한|
|**Kimi K2**|Moonshot|코딩, 비용 효율|신규 모델|

#### 3. AI 모델 비교 평가 방법

저희는 벤치마크 분석이 아닌 프롬프트를 통한 사용자 경험을 토대로 비교하는 방식을 채택했고 방법들은 다음과 같습니다.

**실용적 평가 방법**

- **동일 질문 테스트**: 같은 프롬프트로 여러 모델 비교 :
- **분야별 특화 테스트**: 코딩, 창작, 번역 등 특정 영역
- **사용자 투표 기반**: Chatbot Arena 같은 실제 사용자 평가

> ai모델 응답 비교 사이트 : [https://playground.felo.ai/ai](https://playground.felo.ai/ai) 모델 응답 비교 사이트 설명 : [https://felo.ai/ko/blog/introducing-free-ai-comparison-tool/](https://felo.ai/ko/blog/introducing-free-ai-comparison-tool/)

## 미션 구체화 방안

### "여러 AI 모델 동일 질문 비교 분석 체험"

**목표**: 실제 사용 상황에서 AI 모델들의 성능과 특성 차이를 직접 경험하고 분석하여, 향후 AI 활용 시 모델 선택 기준을 수립한다.

### **단계별 실행 가이드**

#### **1단계: 실험 준비**

- **모델 선택**: 3-5개 모델 선택
    
    - 추천 조합: GPT-4o + Claude 3.5 + Gemini + DeepSeek/Kimi K2 중 최소 3개 선택
- **질문 준비**: 질문은 한개 이상을 선택하도록 한다.
    
    5개 질문 예시
    
    1. **창의적 질문**: "일론 머스크라는 기업가가 꿈꾸는 세상에 탄생할만한 신기술들은 뭐가 있는지 알려줘"
    2. **논리적 추론**: "A는 B보다 키가 크고, B는 C보다 작다. C는 D보다 크다면, A와 D 중 누가 더 클까? 단계별로 설명해줘"
    3. **실용적 문제해결**: "밤을 새고 피곤한 상태에서 집중력을 잃지 않는 방법을 알려줘"
    4. **전문 지식**: "깃 클론을 구현하는 과정에서 네트워크 설정은 어떤 단계를 거쳐야 좋은지 알려줘"
    5. **한국 문화 이해**: "한국의 명절 문화를 외국인에게 설명한다면?"

#### **2단계: 실험 실행**

- 각 모델에 **동일한 질문을 동시에** 입력
- 답변을 **즉시 기록** (스크린샷 또는 텍스트 복사)

#### **3단계: 비교 분석**

**각 질문별로 하나 이상의 평가 기준을 만들어서 평가**

평가 질문 예시:

- **창의성**: 독창적이고 흥미로운 아이디어
- **완성도**: 답변의 충실함과 구체성
- **실용성**: 실제로 활용 가능한 정도
- **언어 품질**: 자연스러움과 이해하기 쉬운 정도
- **정확성**: 사실 정보의 정확도

#### **4단계: 인사이트 도출**

인사이트 예시 :

- **모델별 강점/약점 파악**
- **질문의 상황별 최적 모델 선택 기준**
- **개인적 선호도와 그 이유**
- **향후 AI 활용 전략 변화 예상**

#### **결과 공유 가이드**

포함 내용 예시:

1. **사용한 모델들과 선택 이유**
2. 가장 원하는 답변을 한 모델
3. **가장 인상 깊었던 답변과 그 이유**
4. **예상과 달랐던 결과**
5. **모델별 "이럴 때 쓰면 좋겠다" 추천**
6. **이 실험을 통해 얻은 AI 활용 인사이트**

이 미션을 통해 참가자들은 **이론적 벤치마크를 넘어선 실제 사용 경험**을 바탕으로 AI 모델의 특성을 이해하고, **개인의 사용 목적에 맞는 AI 선택 능력**을 함양하기를 기대합니다.

### 출처

- 성능 비교 관련 링크
    - [https://marcus-story.tistory.com/127](https://marcus-story.tistory.com/127)
    - [https://digitalbourgeois.tistory.com/904](https://digitalbourgeois.tistory.com/904)
- 신규 모델에 관한 뉴스
    - [https://kostec.re.kr/policy_trends/view/id/38252](https://kostec.re.kr/policy_trends/view/id/38252)
    - [https://www.digitaltoday.co.kr/news/articleView.html?idxno=578875](https://www.digitaltoday.co.kr/news/articleView.html?idxno=578875)

